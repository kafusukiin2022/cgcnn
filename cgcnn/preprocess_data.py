import os
import csv
import json
import warnings
import argparse
from tqdm import tqdm
from multiprocessing import Pool, cpu_count

import numpy as np
import torch
from pymatgen.core.structure import Structure

# --- ä» data.py å¤åˆ¶è¿‡æ¥çš„è¾…åŠ©ç±» ---
# æˆ‘ä»¬å°†è¿™äº›ç±»å¤åˆ¶è¿‡æ¥ï¼Œè®©è¿™ä¸ªé¢„å¤„ç†è„šæœ¬å¯ä»¥ç‹¬ç«‹è¿è¡Œï¼Œä¸ä¾èµ–å…¶ä»–æ–‡ä»¶ã€‚

class GaussianDistance(object):
    """
    é€šè¿‡é«˜æ–¯åŸºå‡½æ•°æ‰©å±•è·ç¦»ã€‚
    å•ä½: åŸƒ (angstrom)
    """
    def __init__(self, dmin, dmax, step, var=None):
        assert dmin < dmax
        assert dmax - dmin > step
        self.filter = np.arange(dmin, dmax + step, step)
        if var is None:
            var = step
        self.var = var

    def expand(self, distances):
        """
        å¯¹è·ç¦»æ•°ç»„åº”ç”¨é«˜æ–¯æ»¤æ³¢å™¨ã€‚
        """
        return np.exp(-(distances[..., np.newaxis] - self.filter)**2 /
                      self.var**2)


class AtomCustomJSONInitializer(object):
    """
    ä½¿ç”¨JSONæ–‡ä»¶åˆå§‹åŒ–åŸå­ç‰¹å¾å‘é‡ã€‚
    JSONæ–‡ä»¶æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå°†å…ƒç´ åºæ•°æ˜ å°„åˆ°å…¶ç‰¹å¾å‘é‡åˆ—è¡¨ã€‚
    """
    def __init__(self, elem_embedding_file):
        with open(elem_embedding_file) as f:
            elem_embedding = json.load(f)
        elem_embedding = {int(key): value for key, value
                          in elem_embedding.items()}
        self.atom_types = set(elem_embedding.keys())
        self._embedding = {}
        for key, value in elem_embedding.items():
            self._embedding[key] = np.array(value, dtype=float)

    def get_atom_fea(self, atom_type):
        assert atom_type in self.atom_types
        return self._embedding[atom_type]

# --- æ ¸å¿ƒé¢„å¤„ç†å‡½æ•° ---
# å°†å¤„ç†å•ä¸ªCIFæ–‡ä»¶çš„é€»è¾‘å°è£…æˆä¸€ä¸ªå‡½æ•°
def process_single_cif(args):
    cif_id, target, root_dir, max_num_nbr, radius, dmin, step, atom_init_file = args
    
    # é‡æ–°åˆå§‹åŒ–è¿™äº›å¯¹è±¡ï¼Œå› ä¸ºå®ƒä»¬ä¸èƒ½åœ¨è¿›ç¨‹é—´ç›´æ¥å…±äº«
    # æˆ–è€…å¯ä»¥å°†å®ƒä»¬ä½œä¸ºå‚æ•°ä¼ é€’ï¼Œä½†å¯¹äºå½“å‰ç»“æ„ï¼Œé‡æ–°åˆå§‹åŒ–æ›´ç®€å•
    ari = AtomCustomJSONInitializer(atom_init_file)
    gdf = GaussianDistance(dmin=dmin, dmax=radius, step=step)

    cif_path = os.path.join(root_dir, cif_id + '.cif')
    processed_dir = os.path.join(root_dir, 'processed')

    if not os.path.exists(cif_path):
        warnings.warn(f"è­¦å‘Š: æ–‡ä»¶ '{cif_path}' æœªæ‰¾åˆ°ï¼Œè·³è¿‡ã€‚")
        return None # è¿”å› None è¡¨ç¤ºå¤„ç†å¤±è´¥æˆ–è·³è¿‡

    try:
        crystal = Structure.from_file(cif_path)
        
        # è·å–åŸå­ç‰¹å¾
        atom_fea = np.vstack([ari.get_atom_fea(crystal[i].specie.number)
                              for i in range(len(crystal))])

        # è·å–é‚»å±…ä¿¡æ¯
        all_nbrs = crystal.get_all_neighbors(radius, include_index=True)
        all_nbrs = [sorted(nbrs, key=lambda x: x[1]) for nbrs in all_nbrs]
        
        nbr_fea_idx, nbr_fea = [], []
        for nbr in all_nbrs:
            if len(nbr) < max_num_nbr:
                # åŸå§‹è­¦å‘Šåœ¨å¹¶è¡Œç¯å¢ƒä¸­å¯èƒ½ä¸å¥½å¤„ç†ï¼Œè¿™é‡Œç®€åŒ–
                # warnings.warn(f'ID {cif_id} æœªæ‰¾åˆ°è¶³å¤Ÿçš„é‚»å±…æ¥æ„å»ºå›¾ã€‚')
                nbr_fea_idx.append(list(map(lambda x: x[2], nbr)) +
                                   [0] * (max_num_nbr - len(nbr)))
                nbr_fea.append(list(map(lambda x: x[1], nbr)) +
                               [radius + 1.] * (max_num_nbr - len(nbr)))
            else:
                nbr_fea_idx.append(list(map(lambda x: x[2], nbr[:max_num_nbr])))
                nbr_fea.append(list(map(lambda x: x[1], nbr[:max_num_nbr])))

        nbr_fea_idx = np.array(nbr_fea_idx)
        nbr_fea = np.array(nbr_fea)
        nbr_fea = gdf.expand(nbr_fea)

        # è½¬æ¢ä¸º PyTorch å¼ é‡
        atom_fea = torch.Tensor(atom_fea)
        nbr_fea = torch.Tensor(nbr_fea)
        nbr_fea_idx = torch.LongTensor(nbr_fea_idx)
        target_tensor = torch.Tensor([float(target)])

        # ä¿å­˜ä¸º .pt æ–‡ä»¶
        save_path = os.path.join(processed_dir, f'{cif_id}.pt')
        torch.save(((atom_fea, nbr_fea, nbr_fea_idx), target_tensor, cif_id), save_path)
        return cif_id # è¿”å›æˆåŠŸå¤„ç†çš„ID
    except Exception as e:
        warnings.warn(f"å¤„ç†æ–‡ä»¶ '{cif_path}' æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return None


def preprocess_data_parallel(root_dir, max_num_nbr=12, radius=8, dmin=0, step=0.2):
    """
    éå†åŸå§‹æ•°æ®é›†ç›®å½•ï¼Œå°†æ¯ä¸ªCIFæ–‡ä»¶è½¬æ¢ä¸ºPyTorchå¼ é‡å¹¶ä¿å­˜ã€‚
    æ­¤ç‰ˆæœ¬ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†ã€‚

    å‚æ•°:
    ----------
    root_dir: str
        åŸå§‹æ•°æ®é›†çš„æ ¹ç›®å½•è·¯å¾„ã€‚
        (åº”åŒ…å« id_prop.csv, atom_init.json, å’Œæ‰€æœ‰ .cif æ–‡ä»¶)
    max_num_nbr: int
        æ„å»ºæ™¶ä½“å›¾æ—¶çš„æœ€å¤§é‚»å±…æ•°ã€‚
    radius: float
        æœç´¢é‚»å±…çš„æˆªæ­¢åŠå¾„ã€‚
    dmin: float
        æ„å»ºGaussianDistanceçš„æœ€å°è·ç¦»ã€‚
    step: float
        æ„å»ºGaussianDistanceçš„æ­¥é•¿ã€‚
    """
    # 1. æ£€æŸ¥è·¯å¾„å’Œæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    assert os.path.exists(root_dir), f"é”™è¯¯: æ ¹ç›®å½• '{root_dir}' ä¸å­˜åœ¨ï¼"
    id_prop_file = os.path.join(root_dir, 'id_prop.csv')
    assert os.path.exists(id_prop_file), f"é”™è¯¯: '{id_prop_file}' ä¸å­˜åœ¨ï¼"
    atom_init_file = os.path.join(root_dir, 'atom_init.json')
    assert os.path.exists(atom_init_file), f"é”™è¯¯: '{atom_init_file}' ä¸å­˜åœ¨ï¼"

    # 2. åˆ›å»ºç”¨äºå­˜æ”¾é¢„å¤„ç†åæ•°æ®çš„ç›®å½•
    processed_dir = os.path.join(root_dir, 'processed')
    if not os.path.exists(processed_dir):
        os.makedirs(processed_dir)
        print(f"åˆ›å»ºç›®å½• '{processed_dir}' ç”¨äºå­˜æ”¾é¢„å¤„ç†æ•°æ®ã€‚")

    # 3. è¯»å– id_prop.csv æ–‡ä»¶
    with open(id_prop_file) as f:
        reader = csv.reader(f)
        id_prop_data = [row for row in reader]

    # 4. å‡†å¤‡å¹¶è¡Œä»»åŠ¡çš„å‚æ•°åˆ—è¡¨
    tasks = []
    for cif_id, target in id_prop_data:
        tasks.append((cif_id, target, root_dir, max_num_nbr, radius, dmin, step, atom_init_file))

    # 5. ä½¿ç”¨å¤šè¿›ç¨‹æ± è¿›è¡Œå¤„ç†
    print(f"å¼€å§‹ä½¿ç”¨ {cpu_count()} ä¸ªè¿›ç¨‹å¹¶è¡Œé¢„å¤„ç†CIFæ–‡ä»¶...")
    with Pool(cpu_count()) as pool: # ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„CPUæ ¸å¿ƒ
        # å°† tqdm åŒ…è£…åœ¨ pool.imap çš„ç»“æœä¸Šï¼Œè€Œä¸æ˜¯ pool.map
        # è¿™æ · tqdm å°±ä¼šè¿­ä»£åœ°æ¶ˆè´¹ç»“æœï¼Œå¹¶æ˜¾ç¤ºä¸€ä¸ªç»Ÿä¸€çš„è¿›åº¦æ¡
        results = list(tqdm(pool.imap_unordered(process_single_cif, tasks), total=len(tasks),
                            desc="å¤„ç†CIFæ–‡ä»¶"))
    
    successful_count = sum(1 for r in results if r is not None)
    print("="*50)
    print(f"ğŸ‰ é¢„å¤„ç†å®Œæˆï¼æˆåŠŸå¤„ç† {successful_count} ä¸ªæ–‡ä»¶ï¼Œæ‰€æœ‰æ•°æ®å·²ä¿å­˜åˆ° '{processed_dir}' ç›®å½•ä¸‹ã€‚")
    print("="*50)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='CIFæ–‡ä»¶é¢„å¤„ç†è„šæœ¬')
    parser.add_argument('root_dir', type=str,
                        help='åŒ…å«cifæ–‡ä»¶ã€id_prop.csvå’Œatom_init.jsonçš„æ ¹ç›®å½•è·¯å¾„ã€‚')
    parser.add_argument('--max-num-nbr', type=int, default=12,
                        help='æ„å»ºæ™¶ä½“å›¾æ—¶çš„æœ€å¤§é‚»å±…æ•°ã€‚')
    parser.add_argument('--radius', type=float, default=8,
                        help='æœç´¢é‚»å±…çš„æˆªæ­¢åŠå¾„ã€‚')
    parser.add_argument('--dmin', type=float, default=0,
                        help='æ„å»ºGaussianDistanceçš„æœ€å°è·ç¦»ã€‚')
    parser.add_argument('--step', type=float, default=0.2,
                        help='æ„å»ºGaussianDistanceçš„æ­¥é•¿ã€‚')

    args = parser.parse_args()

    # è°ƒç”¨å¹¶è¡Œç‰ˆæœ¬
    preprocess_data_parallel(
        root_dir=args.root_dir,
        max_num_nbr=args.max_num_nbr,
        radius=args.radius,
        dmin=args.dmin,
        step=args.step
    )
